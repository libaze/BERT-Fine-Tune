# BERT 微调项目
## 项目概述
本项目旨在通过微调BERT（Bidirectional Encoder Representations from Transformers）模型，以实现特定任务的性能优化。以下文档提供了关于如何获取BERT预训练权重、使用的数据集。
## 预训练权重
BERT的预训练权重可以通过以下链接下载：
- [bert-base-chinese](https://hf-mirror.com/google-bert/bert-base-chinese)
## 微调后的权重
BERT的微调后的权重可以通过以下链接下载：
- [best_cls_model](https://pan.baidu.com/s/13YHU42LEJno8fdZeljm2Vw?pwd=hndb)
- [best_ner_model](https://pan.baidu.com/s/1da7HBZDjZ98eoGoIzCQORQ?pwd=v7gx)
- [best_qa_model](https://pan.baidu.com/s/1t5ADYtNuxvVYe5kZ7JSeZQ?pwd=f2b6)
- [best_sts_model](https://pan.baidu.com/s/12p20ddVx86M8NvN7v5h3DQ?pwd=z978)
## 数据集
本项目使用的文本语义相似度数据集链接如下：
- [train_pair.json](https://pan.baidu.com/s/1ZggXiOtz50RZRKOTY-EAdw?pwd=g4vv) : train_pair为语义文本相似度任务准备的数据集，包含260w个句子对。
