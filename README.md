# BERT 微调项目
## 项目概述
本项目旨在通过微调BERT（Bidirectional Encoder Representations from Transformers）模型，以实现特定任务的性能优化。以下文档提供了关于如何获取BERT预训练权重、使用的数据集。
## 预训练权重
BERT的预训练权重可以通过以下链接下载：
- [bert-base-chinese](https://hf-mirror.com/google-bert/bert-base-chinese)
## 微调后的权重
BERT的微调后的权重可以通过以下链接下载：
- [best_cls_model](https://pan.baidu.com/s/1TbDFdBPcB9i4kPewUGDfng?pwd=hjf1)
- [best_ner_model](https://pan.baidu.com/s/1v-tn4h-OBHwM13ka0iVGvg?pwd=d5yc)
- [best_qa_model](https://pan.baidu.com/s/1fQ-XwkW57HX150jO5Qv6JQ?pwd=ci7r)
- [best_sts_model](https://pan.baidu.com/s/16S3fnzIc1YXqTca3gPF-xw?pwd=ms8s)
## 数据集
本项目使用的文本语义相似度数据集链接如下：
- [train_pair.json](https://pan.baidu.com/s/1_CTOH6O_UwRcRlFdkB9vFg?pwd=n25r) : train_pair为语义文本相似度任务准备的数据集。
